---
author:
  - name: Pedro Albuquerque
    affiliation: University of Brasilia
    address: >
      Faculdade de Economia, Administracao e Contabilidade,
      Building A-2 - Office A1-54/7,
      Brasilia, DF 70910-900.
    email: pedroa@unb.br
    url: http://pedrounb.blogspot.com/
  - name: Denis Ribeiro do Valle
    affiliation: University of Florida
    address: >
      408 McCarty Hall C,
      PO Box 110339,
      Gainesville, FL 32611-0410.
    email: drvalle@ufl.edu
    url: http://denisvalle.weebly.com/   
  - name: Daijiang Li
    affiliation: University of Florida
    address: >
      408 McCarty Hall C,
      PO Box 110339,
      Gainesville, FL 32611-0410.
    email: daijianglee@gmail.com
    url: http://daijiang.name/

title:
  formatted: "Bayesian LDA for categorical mixture model clustering."
  plain:     "Bayesian LDA for categorical mixture model clustering."
  short:     "\\pkg{Rlda}: Bayesian LDA for categorical clustering."
abstract: >
  The goal of this paper is to describe the Bayesian LDA model for fuzzy clustering based on different types of data (i.e., Multinomial, Bernoulli, and Binomial entries), provide some examples of the use of this model in R, and to introduce a new R package called Rlda. These types of data frequently emerge in fields as disparate as ecology, remote sensing, marketing, and finance. As result, we believe this package will be of broad interest for pattern recognition, particularly fuzzy clustering for categorical data.
keywords:
  # at least one keyword must be supplied
  formatted: [LDA, fuzzy clustering, categorical data, pattern recognition]
  plain:     [LDA, fuzzy clustering, categorical data, pattern recognition]
preamble: >
  \usepackage{amsmath}
  \usepackage{amsfonts}
  \usepackage{bbm}
  \usepackage{calrsfs}
  \usepackage{draftwatermark}
  \usepackage{graphicx}
  \usepackage{wrapfig}
  \usepackage{lscape}
  \usepackage{rotating}
  \usepackage{epstopdf}
  \SetWatermarkText{DRAFT}
  \SetWatermarkScale{1}
  \newcommand{\Za}{\mathcal{Z}}
output: rticles::jss_article

#output: word_document
#http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
bibliography: bibliography.bib
---

#Introduction.
The Latent Dirichlet Allocation model (LDA), first proposed by @blei2003latent, has been extensively used for text-mining in multiple fields. @tsai2011tag used LDA to construct clusters of tags that represent the most common topics in blogs. @lee2010empirical compared LDA against three other text mining methods that are frequently used: latent semantic analysis, probabilistic latent semantic analysis, and the correlated topic model. The limitations of LDA, as identified by these authors, were that the method does not consider relationship between topics and cannot allocate a given word to multiple topics simultaneously as a fuzzy clustering approach does. Despite these limitations, however, LDA continues to be used in multiple disciplines. For instance, @griffiths2004finding used LDA to identify the main scientific topics in a large corpus of the Proceedings of the National Academy of Science articles.  In conservation biology, LDA has been used to identify research gaps in the conservation literature [@westgate2015text]. LDA has also been proposed as a promising method for the automatic annotation of remote sensing imagery [@lienou2010semantic]. In marketing, LDA has been used to extract information from product reviews across 15 firms in five markets over four years, enabling the identification of the most important latent dimensions of consumer decision making in each studied market [@tirunillai2014mining]. Finally, in finance, a stock market analysis system based on LDA was used to combine financial news items together with stock market data to identify and characterize major events that impact the market. This system was then used to make predictions regarding stock market behaviour based on news items identified by LDA [@mahajan2008mining]. 

Despite its success in text mining across multiple fields, LDA is a model that need not be restricted to text-mining. More specifically, LDA can be viewed as a mixture model since each element in the sample can belong to more than one cluster (or state) simultaneously. There are a few examples of LDA being used for other purposes than text-mining. For instance, a modified version of LDA has been extensively used on genetic data to identify populations and admixture probabilities of individuals [@pritchard2000inference]. Similarly, LDA has been used in ecology to identify plant communities from tree data for the eastern United States and from a tropical forest chronosequence [@valle2014decomposing]. 

The aim of this paper is to describe a Bayesian LDA model for mixture model clustering based on different types of data (i.e., multinomial, Bernoulli and Binomial), illustrating its use in a diverse set of examples. The innovative features of this model are that it generalizes LDA for other types of commonly encountered categorical data and it enables the selection of the optimal number of clusters based on a truncated stick-breaking prior approach which regularizes model results. Finally, we provide a package to fit this novel Bayesian LDA model.

This paper is organized as follows. Section 2 describes the mathematical formulation for the Bayesian LDA model and section 3 shows how the model was implemented in R. Sections 4 and 5 present examples of the use of the package and the conclusions, respectively.


#Methods.
In the Bayesian LDA model for fuzzy clustering we postulate that each element is allocated to a single cluster, represented by a latent state variable. Specifically, consider a latent matrix $\mathbf{Z}$ with dimension equals to $L\times C$ where each row represents a sampling unit ($l=1,\dots,L$) and each column a possible state or cluster ($c=1,\dots,C$). The Data Generating Process postulated for this latent matrix is given by:
<!--  
$$
\begin{array}{c}
Z_{lc}\sim Multinomial(n_{l},\theta_{l})
\end{array}
$$
--> 
\begin{equation}
\boldsymbol Z_{l\cdot}\sim Multinomial(n_{l},\boldsymbol\theta_{l})
\label{eq:eq0001}
\end{equation}

\noindent where $n_{l}$ is total number of elements drawn for location $l$ and $\boldsymbol\theta_{l}=(\theta_{l1},\dots,\theta_{lC})$ is a vector of parameters representing the probability of allocation in each cluster. Following Occam's razor, we intend to create the least number of clusters as possible, which is achieved by assuming a truncated stick-breaking prior:
<!--  
$$
\begin{array}{c}
\theta_{lc^{*}}=V_{lc^{*}}\displaystyle\prod_{c=1}^{c^{*}-1}(1-V_{lc})
\end{array}
$$
--> 
\begin{equation}
\theta_{lc}=V_{lc}\displaystyle\prod_{c^{*}=1}^{c-1}(1-V_{lc^{*}})
\label{eq:eq0002}
\end{equation}


\noindent where $V_{lc}\sim Beta(1,\gamma)$ for $c=1,\dots,C-1$ and $V_{lC}=1$ by definition. This truncated stick-breaking prior will force the elements to be aggregated in the minimum number of clusters, given that $\theta_{lc^{*}}$ is stochastically exponentially decreasing.

In the second hierarchical level, we consider a matrix $\mathbf{Y}$ with dimension equal to $L\times S$ where each row represents a sampling unit (e.g., locations, firms, individuals, plots) and each column a variable that describes these elements. In the Bayesian LDA model for fuzzy clustering, after integrating over the latent vector $\mathbf{Z}_{l\cdot}$, $Y_{ls}$ can follow one of these distributions:
<!--  
$$
\begin{array}{lll}
\mathbf{Y}_{l\cdot}|Z_{lc}=c^{*}&\sim& Multinomial(n_{l},\phi_{c^{*}})\\
\mathbf{Y}_{l\cdot}|Z_{lc}=&c^{*}&\sim& Bernoulli(\phi_{c^{*}})\\
\mathbf{Y}_{l\cdot}|Z_{lc}=&c^{*}&\sim& Binomial(n_{ls},\phi_{c^{*}})
\end{array}
$$
--> 
\begin{equation}
\begin{cases}
\mathbf{Y}_{l\cdot}\sim Multinomial(n_{l},\boldsymbol\theta_{i}^{t}\Phi)\\
Y_{ls}\sim Bernoulli(\boldsymbol\theta_{i}^{t}\boldsymbol\phi_{s})\\
Y_{ls}\sim Binomial(n_{ls},\boldsymbol\theta_{i}^{t}\boldsymbol\phi_{s})
		\label{eq:eq0003}
\end{cases}		
\end{equation}


\noindent for $l=1,\dots,L$ and $s=1,\dots,S$ possible variables. $Y_{ls}$ represents a random variable, $\boldsymbol Y_{l\cdot}$ is a vector with these random variables for location $l$, $n_{l}$ is the total number of elements in sampling unit $l$, $n_{ls}$ is the total number of elements in sampling unit $l$ and variable $s$. In these models,  $\boldsymbol\phi_{s}=(\phi_{1s},\dots,\phi_{Cs})$ is a vector of parameters, while $\boldsymbol\Phi$ is a  $C\times S$ matrix of parameters, given by:

$$
\mathbf{\Phi}=\begin{bmatrix}
    \phi_{11} & \phi_{12} & \dots  & \phi_{1S} \\
    \phi_{21} & \phi_{22} & \dots  & \phi_{2S} \\
    \vdots & \vdots  & \ddots & \vdots \\
    \phi_{C1} & \phi_{C2} & \dots  & \phi_{CS} 
\end{bmatrix}
$$

In the last step, we specify the priors for $\phi_{cs}$. For the multinomial model, we adopt a Dirichlet prior (i.e. $\boldsymbol\phi_{c}\sim Dirichlet(\boldsymbol\beta)$ where $\boldsymbol\beta=(\beta_{1},\dots,\beta_{S})$ is the hyperparameter vector). For the Bernoulli and Binomial representations, we assume that $\phi_{cs}$ comes from a Beta distribution, (i.e., $\phi_{cs}\sim Beta(\alpha_{0},\alpha_{1})$).

These models are fit using Gibbs Sampling where parameter draws are iteratively made from each full conditional distribution. From a conceptual perspective, all of these models assume the following matrix decomposition:

\begin{equation}
\mathbb{E}[\mathbf{Y}_{L\times S}]=\mathbf{K}\circ[\Theta_{L\times C}\Phi_{C\times S}]
	\label{eq:eq0004} 
\end{equation}

where $\mathbf{K}$ is a matrix of constants and $\circ$ is the Hadamard product. Sparseness is ensured by forcing large $c$ in the $\Theta_{L\times C}$ matrix to be close to zero. For the multinomial model, the $\mathbf{K}$ matrix contains the total number of elements in each row whereas for the Bernoulli model, this matrix is equal to the identity matrix. Finally, for the Binomial model, the $\mathbf{K}$ matrix has the total number of trials of each binomial distribution (i.e., $n_{ls}$). Although there are many ways matrices can be decomposed, the key characteristic of this particular form of matrix decomposition is that each row of $\mathbf{\Theta}$ is comprised of probabilities that sum to one. As a result, one can interpret $\Phi_{C\times S}$ as the matrix that contain the "pure" features of the data, which are then mixed by the matrix $\Theta_{L\times C}$ and multiplied by $\mathbf{K}$ to generate the expected data.

## Full Conditional Distributions - FCD.
### Bernoulli model.

The probability of community membership status $Z_{ls}$ is given by: 

\begin{equation}
\begin{array}{lll}
p(Z_{ls}=c^{*}|...)&=&\frac{\theta_{lc^{*}}\phi_{c^{*}s}^{y_{ls}}(1-\phi_{c^{*}s})^{1-y_{ls}}}{\displaystyle\sum_{c=1}^{C}\theta_{lc}\phi_{cs}^{y_{ls}}(1-\phi_{cs})^{1-y_{ls}}}
\end{array}
\label{eq:eq0005} 
\end{equation}

Therefore, $Z_{ls}$ can be drawn from a categorical distribution. The FCD for $V_{lc}$ is given by:

$$
p(V_{lc}|...)=Beta(N_{lc}+1,N_{l(c^{*}>c)}+\gamma)
$$

\noindent where $N_{lc}$ is the total number of elements in location $l$ classified into cluster $c$, and $N_{l(c^{*}>c)}$ is the total number of elements in location $l$ classified in clusters larger than $c$. These quantities are given by $N_{lc}=\sum_{s=1}^{S}\mathbbm{1}(z_{ls}=c)$ and $N_{l(c^{*}>c)}=\sum_{s=1}^{S}\sum_{c^{*}=c+1}^{C}\mathbbm{1}(z_{ls}=c^{*})$, respectively. Finally, the FCD for $\phi_{cs}$ is given by:

$$
p(\phi_{cs}|...)=Beta(N_{cs}^{(1)}+\alpha_0,N_{cs}^{(0)}+\alpha_1)
$$

\noindent where $N_{cs}^{(1)}$ is the number of elements assigned to cluster $c$ and for which $Y_{ls}=1$ (i.e., $\sum_{l=1}^L\mathbbm{1}(z_{ls}=c,Y_{ls}=1)$) and $N_{cs}^{(0)}$ is the number of elements assined to cluster $c$ and for which $Y_{ls}=0$ (i.e., $\sum_{l=1}^L\mathbbm{1}(z_{ls}=c,Y_{ls}=0)$).

### Binomial model.
For this model, we have $n_{ls}$ elements for each sampling unit $l$ and community $z$. The $i$-th element is denoted as $Z_{ils}$ and its probability is similar to the one for the Bernoulli model:

\begin{equation}
\begin{array}{lll}
p(Z_{lc}=c^{*}|...)&=&\frac{\theta_{lc^{*}}\phi_{c^{*}s}^{x_{ils}}(1-\phi_{c^{*}s})^{1-x_{ils}}}{\displaystyle\sum_{c=1}^{C}\theta_{lc}\phi_{cs}^{x_{ils}}(1-\phi_{cs})^{1-x_{ils}}}
\end{array}
\label{eq:eq0006} 
\end{equation}

\noindent where $x_{ils}$ are binary random variables such that $\sum_{i=1}^{n_{ls}}x_{ils}=y_{ls}$. Therefore, $Z_{ils}$ can be drawn from a multinomial distribution. The FCD for $\phi_{cs}$ is given by:

\begin{equation}
\begin{array}{lll}
p(\phi_{cs}|...)= Beta\left(\displaystyle\sum_{l=1}^{L}\displaystyle\sum_{i=1}^{n_{ls}}\mathbbm{1}(x_{ils}=1,z_{ils}=c)+\alpha_{0},\displaystyle\sum_{l=1}^{L}\displaystyle\sum_{i=1}^{n_{ls}}\mathbbm{1}(x_{ils}=0,z_{ils}=c)+\alpha_{1}\right)
\end{array}
\label{eq:eq0007} 
\end{equation}

Finally, the FCD for $V_{lc}$ is given by: 

\begin{equation}
p\left(V_{lc}|...\right) = Beta(N_{lc}+1,N_{l(c^{*}>c)}+\gamma)
	\label{eq:eq0008} 
\end{equation}

\noindent where $N_{lc}$ is the total number of elements in location $l$ classified into cluster $c$ and $N_{l(c^{*}>c)}$ is the total number of elements in location $l$ classified in clusters larger than $c$. These quantities are given by $N_{lc}=\sum_{s=1}^{S}\sum_{i=1}^{n_{ls}}\mathbbm{1}(z_{ils}=c)$ and $N_{l(c^{*}>c)}=\sum_{s=1}^{S}\sum_{i=1}^{n_{ls}}\sum_{c^{*}=c+1}^{C}\mathbbm{1}(z_{ils}=c^{*})$.


### Multinomial  model.
For the Multinomial case, if unit $i$ in location $l$ is associated with variable $s$ (i.e., $x_{il}=s$ such that $y_{ls}=\sum_{i=1}^{n_{l}}\mathbbm{1}(x_{il}=s)$), we have that:

\begin{equation}
p(Z_{il}=c^{*}|...)=\frac{\theta_{lc^{*}}\phi_{sc^{*}}}{\left(\theta_{1l}\phi_{s1}+\dots+\theta_{Cl}\phi_{sC}\right)}
	\label{eq:eq0009} 
\end{equation}

Therefore, $Z_{il}$ can be sampled from a categorical distribution. Since we assumed a conjugate prior for $\boldsymbol\phi_{c^{*}}$ with $c^{*}\in\{1,\dots,C\}$ the Full Conditional Distribution for this vector of parameters is a straight-forward Dirichlet distribution: 

\begin{equation}
p(\boldsymbol\phi_{c^{*}}|...)=Dirichlet([n_{c^{*}1}+\beta_{1},\dots,n_{c^{*}S}+\beta_{S}])
	\label{eq:eq0010} 
\end{equation}

\noindent where $n_{c^{*}s}$ is the total number of observations classified in cluster $c^{*}$ in all locations for the $s$-th variable (i.e., $n_{c^{*}s}=\sum_{l=1}^{L}\sum_{i=1}^{n_{l}}\mathbbm{1}(z_{il}=c^{*},x_{il}=s)$).  

Finally, the FCD for $V_{lc^{*}}$ is given by: 

\begin{equation}
  p(V_{lc^{*}}|...)=Beta(N_{lc^{*}}+1,N_{l(c>c^{*})} +\gamma)
	\label{eq:eq0011} 
\end{equation}


\noindent where $N_{lc^{*}}$ is the total number of elements in observation $l$ classified into cluster $c^{*}$ and $N_{l(c>c^{*})}$ is the total number of elements in observation $l$ classified in clusters larger than $c^{*}$. These quantities are given by $N_{lc^{*}}=\sum_{i=1}^{n_{l}}\mathbbm{1}(z_{il}=c^{*})$ and $N_{l(c>c^{*})}=\sum_{i=1}^{n_{l}}\sum_{c=c^{*}+1}^{C}\mathbbm{1}(z_{il}=c)$.


#The Rlda package.
We found four other packages that can fit the Latent Dirichlet Allocation model. @hornik2011topicmodels developed the \pkg{topicmodels} package for which there are two LDA implementations, one using the variational inference (as described in @blei2003latent) and the other implementation using Gibbs Sampling based on @phan2013gibbslda. Similarly, @jones2016 proposed the \pkg{textmineR} which relies on Gibbs Sampling to estimate the topics in a corpus structure. @chang2012lda developed the \pkg{lda} package which includes the mixed-membership stochastic blockmodel [@airoldi2008mixed], supervised Latent Dirichlet Allocation - sLDA [@mcauliffe2008supervised] and Correspondence-Latent Dirichlet Allocation - corrLDA[@blei2003modeling]. Finally, more recently, @roberts2014stm created the \pkg{stm} which has some unsupervised functions to determine the optimal number of clusters. This unsupervised method is based on likelihood metrics obtained by the EM Algorithm, which are then used within a backward model selection approach.

None of these packages adopt the truncated stick-breaking prior which enables the selection of the optimal number of clusters and regularizes model results. Furthermore, none of them use others distributions besides the Multinomial outcome for the dependent variable. Thus, our Rlda package complements current LDA approaches already available in R. 


#Examples.
In this section we present some applications for Latent Dirichlet Allocation in marketing, remote sensing and ecology.

##Marketing.

```{r,echo=FALSE}
#Load RData (Example 1)
rm(list=ls())
load("Example1.RData")
```

The first application considers the classical LDA for a Multinomial entry in the field of Marketing. More specifically, we are interested in characterizing firms based on their consumers' complaints.

It is well known that attracting a new customer is often considerably costlier than keeping current customer [@kotler2006principles]. For this reason, firms can better retain their customers if they pay careful attention to their consumers' complaints and work to solve them in a satisfactory way. 

The data come from the 2015 [Consumer Complaint Database](http://catalog.data.gov/dataset/consumer-complaint-database) and consist of 
complaints received by the **Bureau of Consumer Financial Protection** in US regarding financial products and services. In this example, we work only with credit card complaints. This dataset contains information on the number of complaints for each firm ($L=$ `r length(unique(complaints$Company))`), categorized according to the specific type of issue ($S=$ `r length(unique(complaints$Issue))`). Examples of issues include  billing disputes, identity theft / fraud, and unsolicited issuance of credit card.  Each sampling unit in this case represents a firm and each variable the total number of complaints associated with each issue. 

The characterization of firms provided by our analysis can be useful to reveal communalities and differences across different firms. This can then be used by managers to identify and potentially adopt the solutions that are employed by other firms to deal with these issues.

To use the \pkg{Rlda} package for the Multinomial entry is necessary to create a matrix where each cell represents the total number of cases observed for each variable and sampling unit.


```{r, eval=FALSE}
library(Rlda)
#Read the Complaints data
data(complaints)
#Create the abundance matrix
library(reshape2)
mat1<-dcast(complaints[,c("Company","Issue")], 
            Company~Issue, length, 
            value.var="Issue")
#Create the rowname
rownames(mat1)<-mat1[,1]
#Remove the ID variable
mat1<-mat1[,-1]
```



To use the \code{lda_multinomial} method we need to specify some arguments:

```{r,eval=FALSE}
#Set seed
set.seed(9292)
#Hyperparameters for each prior distribution
beta<-rep(1,ncol(mat1))
gamma<-0.01
#Execute the LDA for the Multinomial entry
res<-lda_multinomial(data=mat1,n_community=30,beta,gamma,n_gibbs=1000,
                     ll_prior=TRUE,display_progress=FALSE)
```

In the above code we set the maximum number of clusters to 30 and the number of Gibbs Sampling iterations to 1000. Furthermore, we ask that the algorithm output the sum of the log-likelihood and the log-prior. Finally, we chose not to display the progress bar.

We can visually evaluate the convergence by examining Figure \ref{figure:fig001}:


```{r,eval=FALSE}
#Get the logLikelihood
ll<-res[["logLikelihood"]]
#Plot the log-likelihood
plot(ll,type="l",xlab="Interations",
                 ylab="Log-likelihood")
```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
#Get the logLikelihood
ll<-res[["logLikelihood"]]
pdf("plot001.pdf", width=4, height=3)
#Plot the log-likelihood
plot(ll,type="l",xlab="Interations",ylab="Log-likelihood",
     cex.lab=0.7,cex.axis=0.7, cex.main=0.7, cex.sub=0.7)
invisible(dev.off())
```

\begin{figure}[htbp]
  \centering
  \includegraphics{plot001.pdf}
  \caption{Log-likelihood interations.}
  \label{figure:fig001}
\end{figure}

Samples of our parameter estimates are given in the \code{Theta} and \code{Phi} matrices. Each line in these matrices contains the result of a Gibbs iteration. This way, we can get parameter estimates based on the columns of these matrices after discarding iterations prior to the *burn-in phase*.

```{r,eval=TRUE}
#Get the Theta Estimate
Theta<-res[["Theta"]]
#Burnout
Theta<-colMeans(Theta[300:1000,])
#Create the matrix
Theta<-matrix(Theta,nrow = nrow(mat1),ncol=30)
#Rownames
rownames(Theta)<-rownames(mat1)
```

The \code{Theta} matrix has a sparse structure since our truncated stick-breaking prior tends to reduce the total number of dominant clusters. Each cell contains the estimated probability of the $l$-th firm being allocated to cluster $c$.

To describe each cluster, we can examine the \code{Phi} matrix:

```{r,eval=TRUE}
#Get the Phi Estimate
Phi<-res[["Phi"]]
#Burnout
Phi<-colMeans(Phi[300:1000,])
#Create the matrix
Phi<-matrix(Phi,nrow =30,ncol=ncol(mat1))
#Colnames
colnames(Phi)<-colnames(mat1)
#Rownames
rownames(Phi)<-paste0("Cluster ",seq(1,30))
#Get the most likely issues
ids<-which(Phi > 0.2, arr.ind = TRUE)
```

The most likely issues in each cluster are summarized as follow:

```{r xtable1,echo=FALSE, results="asis"}
#Selected elements
PhiSel<-Phi[ids]
matDes<-as.data.frame(cbind(rownames(Phi)[ids[,1]],colnames(Phi)[ids[,2]]))
#Names
colnames(matDes)<-c("Cluster","Issue")
#Remove text
matDes$Cluster<-trunc(as.numeric(gsub("Cluster ","",matDes$Cluster)))
#Sort
matDes <- matDes[order(matDes$Cluster, matDes$Issue),]
rownames(matDes)<-NULL
library(xtable)
tab <- xtable(matDes,digits=c(0,0,0),caption="Clusters description.")
print(tab,comment=FALSE,
      include.rownames=FALSE,
      size="\\fontsize{9pt}{10pt}\\selectfont")
```

Based on that information we can now analyze the firms with the highest probability to belong to these clusters:

```{r xtable2, echo=FALSE, warning=FALSE, results="asis"}
suppressMessages(library(reshape2))
#Get the Clusters
idClusters<-unique(matDes[,"Cluster"])
firms<-Theta[,idClusters]
#Sort the data
firms <- as.data.frame(firms[order(-firms[,1], -firms[,2]),])
#Create colNames
colnames(firms)<-paste0("Cluster ",idClusters)
#Keep only the positive probability
firms<-firms[which(firms[,1]>0.01 | firms[,2]>0.01),]
#Create the firm name
firms$Firm<-rownames(firms)
#melt
namesMelt<-melt(firms,id="Firm")
#Colnames
colnames(namesMelt)<-c("Firm","Cluster","Probability")
#Print
tab2 <- xtable(namesMelt,digits=c(0,0,0,4),
      caption="Probability of belonging for each company.")
print.xtable(tab2,comment=FALSE,include.rownames=FALSE,size="\\fontsize{9pt}{10pt}\\selectfont")
```


The results are consistent with the type of product studied, in this case *Credit card*. It is interesting to note that the Cluster 7 is mostly represented by financial services corporation such as U.S. Bancorp and Amex. This cluster is mostly represented by *Advertising and marketing* and *Rewards* issues which may represent a larger cluster of complaints, for instance *Deceptive advertising complaints*.

Managers of those companies can use this information to create, for example, a specific department to solve this disputes and/or clarify their consumers about the *Advertising* and *Rewards* rules and conditions, aiming to avoid misunderstood about the programs.

Cluster 17 on the other hand is mostly represented by firms with *Closing/Cancelling account* complaints and its most representative firm is Discover which is also a financial services corporation but, differently of the firms with large participation in Cluster 7, this firm has complaints about *Closing/Cancelling account*. For these companies that belong to Cluster 17 they could focus their attention in provide easy steps to *Closing/Cancelling account* in a way to avoid more complaints and improve their Customer Relationship Management (CRM).

The final cluster is represented by financial services corporation that act in most part on web, and its representative firm is PayPal Holdings, Inc. The most likely issue associated with these companies that belongs to Cluster 26 is *Unsolicited issuance of credit card*. 




<!--http://badfinance.org/review/1650668/paypal-credit-card-->
These results can be explained by a change in the PayPal operations. The complaints are related with the type of account provided by PayPal which were converted from standard PayPal account to a revolving credit account and some consumers claims to be unaware of this change, and again, a good communication system between the company and its consumers could avoid and easily solve complaints about those issues.


##Remote Sensing.
```{r,echo=FALSE, eval=FALSE}
#Load RData (Example 2)
rm(list=ls())
load("Example2.RData")
```

Because pixels in remote sensing imagery are often large enough to encompass different substances within each pixel, there has been great interest in the development of methods that enable researchers to estimate the proportion of the constituent materials. Indeed, numerous spectral unmixing algorithms have already been developed in the literature, with multiple approaches used for the dimension reduction, endmember determination, and inversion stages [@keshava2003survey]. 

The key characteristics of the method that we propose here is that it is an unsupervised method (i.e., it does not require the a priori determination of endmembers) that enforces parsimony through our truncated stick-breaking prior. Differently from many of the currently existing methods for spectral unmixing, our model explicitly acknowledges the discreteness of the digital numbers used in remote sensing systems and the range of possible values these numbers can take.

In our example, we rely on Landsat TM 5 imagery from 2010 of the Iquitos-Nauta road in the Peruvian Amazon. This area has multiple land-use land-cover (LULC) types and is the site in which we have studied the effect of these different LULC types on the malaria vector *Anopheles darlingi* [@dennis].

```{r, eval=FALSE}
library(Rlda)
data("Landsat")
#Define the hyperparameters
a.phi  <- 0.1
b.phi  <- 9.9
ngibbs <- 10000
#Execute the Binomial LDA
z <- lda_binomial(data= Landsat,pop= max2, n_community=5,
                  alpha0= a.phi,alpha1= b.phi,gamma= 1,n_gibbs= ngibbs,
                  ll_prior=TRUE,display_progress=FALSE)
```

In a similar way as presented in the Marketing example, we can get the \code{Phi} and \code{Theta} matrices:

```{r, eval=FALSE}
#Total number of gibbs simulations
ngibbs <- 10000
#Sequence
seq1 <- floor(seq(from=(2/3)*ngibbs,to=ngibbs,length.out=1000))
#Theta matrix
theta1 <- z$Theta[seq1,] 
#Phi matrix
phi1   <- z$Phi[seq1,]
```

Using these data, it is possible to construct some predictions to the Iquitos-Nauta road in the Peruvian Amazon for each cluster:

```{r, eval=FALSE, echo=FALSE}
phi1    <- apply(phi1,2,mean)
theta1  <- apply(theta1,2,mean)
ncommun <- 5
ind<-grep('b',colnames(Landsat))
nbands<-length(ind);
nloc<-length(theta1)/ncommun
phi1<-matrix(phi1,nbands,ncommun)
theta1<-matrix(theta1,nloc,ncommun)
```

```{r,echo=FALSE,eval=FALSE, message=FALSE, warning=FALSE}
dat=read.csv('rs data.csv',as.is=T)
#create a matrix with all possible combinations of proportions
seq1=seq(from=0,to=1,by=0.05)
combo=expand.grid(p1=seq1,p2=seq1,p3=seq1,p4=seq1)
cond=apply(combo,1,sum)<= 1
combo1=combo[cond,]
combo1$p5=1-apply(combo1,1,sum)

#calculate probability
probs=data.matrix(combo1)%*%data.matrix(t(phi1))

#find best proportions
nbands=7
nomes=paste('b',1:nbands,sep='')
dat1=data.matrix(dat[,nomes])

tmp=apply(dat1,2,range)
dat2=dat1-matrix(tmp[1,],nrow(dat1),nbands,byrow=T)
tmp=apply(dat2,2,range)
max1=tmp[2,]

#let's change the range of our data to start at zero. 
#Variation outside this range does not help us explain much
tmp=apply(dat1,2,range)
dat2=dat1-matrix(tmp[1,],nrow(dat1),nbands,byrow=T)
tmp=apply(dat2,2,range)
max2=matrix(max1,nrow(probs),length(max1),byrow=T)

#find which proportion of communities yield the highest probability
res=matrix(NA,nrow(dat2),5)
for (i in 1:nrow(dat2)){
  rasc=matrix(dat2[i,],nrow(probs),ncol=ncol(dat2),byrow=T)
  llikel=dbinom(rasc,size=max2,prob=probs,log=T)
  fim=apply(llikel,1,sum)
  ind=which(fim==max(fim))
  res[i,]=as.numeric(combo1[ind,])
}
colnames(res)=paste('prop',1:5,sep='')
res1=cbind(dat$POINT_X,dat$POINT_Y,res)
colnames(res1)[1:2]=c('x','y')
```

```{r,echo=FALSE,eval=FALSE}
#Load RData (Example 2)
rm(list=ls())
load("Example22.RData")
rm(list= ls()[!(ls() %in% c('res1'))])
res1<-as.data.frame(res1)
library(ggmap)


#Original Data
#sbbox <- c(lon =-73.1,lat = -4.4)
#sq_map <- get_map(location = sbbox, zoom = 9, maptype = "satellite", source = "google")
#plot(sq_map)
#ggmap(sq_map)

pdf("plot002.pdf")
par(mfrow=c(3,2))
#pdf("plot0021.pdf")
#Cluster 1
brks <- with(res1, seq(min(prop1), max(prop1), length.out = 100))
grps <- with(res1, cut(prop1, breaks = brks, include.lowest = TRUE))
#cores <- terrain.colors(99)[grps]
cores <- rainbow(99)[grps]


#FUN <- colorRamp(c("red","blue"))
#P1 <- with(res1, (prop1 - min(prop1)) / diff(range(prop1)))
#cols <- FUN(P1)
#cores=rgb(cols, maxColorValue=256)

plot(res1$x,res1$y,pch=19,col=cores,cex=0.8,
     xlim=c(min(res1$x),675500),xlab = "",ylab="",main="Cluster 1")
#invisible(dev.off())

#pdf("plot0022.pdf")
#Cluster 2
brks <- with(res1, seq(min(prop2), max(prop2), length.out = 100))
grps <- with(res1, cut(prop2, breaks = brks, include.lowest = TRUE))
#cores <- terrain.colors(99)[grps]
cores <- rainbow(99)[grps]

#FUN <- colorRamp(c("red","blue"))
#P2 <- with(res1, (prop2 - min(prop2)) / diff(range(prop2)))
#cols <- FUN(P2)
#cores=rgb(cols, maxColorValue=256)
plot(res1$x,res1$y,pch=19,col=cores,cex=0.8,
     xlim=c(min(res1$x),675500),xlab = "",ylab="",main="Cluster 2")
#invisible(dev.off())

#pdf("plot0023.pdf")
#Cluster 3
brks <- with(res1, seq(min(prop3), max(prop3), length.out = 100))
grps <- with(res1, cut(prop3, breaks = brks, include.lowest = TRUE))
#cores <- terrain.colors(99)[grps]
cores <- rainbow(99)[grps]
#FUN <- colorRamp(c("red","blue"))
#P3 <- with(res1, (prop3 - min(prop3)) / diff(range(prop3)))
#cols <- FUN(P3)
#cores=rgb(cols, maxColorValue=256)
plot(res1$x,res1$y,pch=19,col=cores,cex=0.8,
     xlim=c(min(res1$x),675500),xlab = "",ylab="",main="Cluster 3")
#invisible(dev.off())

#pdf("plot0024.pdf")
#Cluster 4
brks <- with(res1, seq(min(prop4), max(prop4), length.out = 100))
grps <- with(res1, cut(prop4, breaks = brks, include.lowest = TRUE))
#cores <- terrain.colors(99)[grps]
cores <- rainbow(99)[grps]
#FUN <- colorRamp(c("red","blue"))
#P4 <- with(res1, (prop4 - min(prop4)) / diff(range(prop4)))
#cols <- FUN(P4)
#cores=rgb(cols, maxColorValue=256)
plot(res1$x,res1$y,pch=19,col=cores,cex=0.8,
     xlim=c(min(res1$x),675500),xlab = "",ylab="",main="Cluster 4")
#invisible(dev.off())

#pdf("plot0025.pdf")
#Cluster 5
brks <- with(res1, seq(min(prop5), max(prop5), length.out = 100))
grps <- with(res1, cut(prop5, breaks = brks, include.lowest = TRUE))
#cores <- terrain.colors(99)[grps]
cores <- rainbow(99)[grps]
#FUN <- colorRamp(c("red","blue"))
#P5 <- with(res1, (prop5 - min(prop5)) / diff(range(prop5)))
#cols <- FUN(P5)
#cores=rgb(cols, maxColorValue=256)
plot(res1$x,res1$y,pch=19,col=cores,cex=0.8,
     xlim=c(min(res1$x),675500),xlab = "",ylab="",main="Cluster 5")


invisible(dev.off())
```


\begin{figure}[htbp]
  \centering
  \includegraphics[width=17cm,height=12cm,keepaspectratio]{plot002.pdf}
  \caption{Clusters - Iquitos-Nauta road.}
  \label{figure:fig002}
\end{figure}

We can note different level of colors in the region which also vary depending on the cluster chosen (the full high-resolution figures are presented in the final of the paper). This similarity between regions is due to the band's share and was modeled by the Binomial LDA. In other words, some regions with red color represent regions whose can be similar with respect to the LDA results and can represent villages, streets, etc. For example, it is easy to see in Cluster 5 the roads and deforested areas looking the pattern presented. Hence, the method can be used to classify regions in an unsupervised way to solve pixel unmixing problems.

##Ecology.
```{r,echo=FALSE}
#Load RData (Example 3)
rm(list=ls())
load("Example3.RData")
```
In many ecological studies, it is not possible to determine the total number of individuals per species in each sampling unit. As a result, these data are often summarized into binary presence/absence matrices (1 and 0, respectively) [@pearce2006modelling].

In this example we used \code{data("SPDATA")} from \pkg{PresenceAbsence} package, which includes presence/absence information on 13 species at 386 forested locations [@moisen2006predicting]. We analyze these data using the \code{lda_bernoulli} function. 

```{r, eval=FALSE,message=FALSE,warning=FALSE}
library(PresenceAbsence)
#Read SPDATA data
data(SPDATA)
PresAbs<-SPDATA[,1:2]
#Location
library(data.table)
PresAbs <- data.table(PresAbs)
PresAbs[, Location := sequence(.N), by = c("SPECIES")]
#Create the binary matrix
library(reshape2)
mat1<-dcast(PresAbs, 
            Location~SPECIES,  
            value.var="OBSERVED")
#Remove the Location variable
matPres<-as.data.frame(mat1[,-1])
```

We use the following arguments for the \code{lda_bernoulli} function:

```{r,eval=FALSE}
#Set seed
set.seed(9842)
#Hyperparameters for each prior distribution
gamma <-0.01
alpha0<-0.01
alpha1<-0.01
#Execute the LDA for the Binomial entry
res<-lda_bernoulli(matPres,10,
                           alpha0,alpha1,gamma,
                           5000,TRUE,FALSE)
```

We can visually evaluate the cluster distribution across species, after the *burn-in phase* in Figure \ref{figure:fig003}:

\newpage

```{r,echo=FALSE, message=FALSE, warning=FALSE}
#Get the Phi Estimate
Theta<-res[["Theta"]]
#Burnout
Theta<-colMeans(Theta[1000:5000,])
#Create the matrix
Theta<-matrix(Theta,nrow = ncol(matPres),ncol=10)
#Rownames
rownames(Theta)<-colnames(matPres)
colnames(Theta)<-paste("Cluster ",seq(1,ncol(Phi)))
#Color
library(RColorBrewer)
myColor<-brewer.pal(n = 10, name = "RdBu")
#Labels
strClusters<-colnames(Theta)
pdf("plot003.pdf", width=10, height=10)
stars(Theta,col.segments=myColor,scale=TRUE,
      draw.segments=TRUE,ncol=4,flip.labels=FALSE,cex=0.6)
legend('bottomright',strClusters,ncol=5,
       fill=myColor,cex=0.6,bty='n')
invisible(dev.off())
```

\begin{figure}[htbp]
  \centering
  \includegraphics[width=12cm,height=8cm,keepaspectratio]{plot003.pdf}
  \caption{Cluster distribution across species.}
  \label{figure:fig003}
\end{figure}

In this type of graph each slice size is proportional to the probability of belonging, and it is possible to note that some species of trees are more or less associated with some clusters. For example, QUGA species is more associated with Clusters 7 and 2 as well ACGR3 species.

#Conclusion.
The goal of this paper was describe the Bayesian LDA model for fuzzy clustering based on different types of data, specifically we demonstrated how to use the model for Multinomial entry in the Marketing example, Binomial trial using the Landsat dataset and Bernoulli trial using ecological data.

Also, we present the results in three different ways showing the flexibility to use the results and represent the information numerically and/or visually which is important when we faced large datasets and a great number of possible clusters.
  
One of the main properties of the model presented here is the fact that this model adopts the truncated stick-breaking prior which enables the selection of the optimal number of clusters which regularizes model results. The next step in the development of the model presented here is the possibility to work with explanatory variables in the same structure described in this article, which can be useful not only to model the cluster but also explain how the clusters are created.

#Appendix. {-}
Here in the Appendix we will show how the Full Conditional Distributions were constructed. Consider initially the Full Conditional Distributions for $Z_{lc}=c^{*}$ in all three cases the kernel of the distribution derived is the same:

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\zeta_{c^{*}},\boldsymbol\theta_{l})&=&\kappa\left[\phi_{c^{*}1}^{\mathbbm{1}(Y_{l1}=s,Z_{lc}=c^{*})}\times\dots\times \phi_{c^{*}S}^{\mathbbm{1}(Y_{lS}=s,Z_{lc}=c^{*})}\right]\\[.5em]
&&\times\left[\theta_{l1}^{\mathbbm{1}(Z_{l1}=c^{*})}\times\dots\times\theta_{lC}^{\mathbbm{1}(Z_{lC}=c^{*})}\right]\\[.5em]
&=&\kappa\zeta_{c^{*}s}\theta_{lc^{*}}
\end{array}
\label{ap:eq0001} 
\end{equation*}

\noindent where $\mathbbm{1}(Y_{l1}=s,Z_{lc}=c^{*})$ is the indicator function which assumes one only for the $l$-th observation, $s$-th variable and has been identified as belong to cluster $c^{*}$. In a similar way, $\mathbbm{1}(Z_{lC}=c^{*})$ assumes one only for observations classified as belong to cluster $c^{*}$.

Since $Z_{lc}$ is a categorical random variable with support in $\Za=(1,2,\dots,C)$ the sum of all probabilities for all elements must one, then the constant $\kappa$ is given by:

\begin{equation*}
\begin{array}{lll}
\kappa&=&\left(\theta_{l1}\zeta_{1s}+\dots+\theta_{lC}\zeta_{Cs}\right)^{-1}
\end{array}
\label{ap:eq0002} 
\end{equation*}

Then, each category for $Z_{lc}$ can be draw from a categorical distribution with probabilities equal to $\kappa\zeta_{c^{*}s}\theta_{lc^{*}}$ with $c^{*} \in \Za$, where $\zeta_{c^{*}s}=\phi_{sc^{*}}$ in the multinomial case and $\zeta_{c^{*}s}=\phi_{c^{*}s}^{y_{ls}}(1-\phi_{c^{*}s})^{1-y_{ls}}$ and $\zeta_{c^{*}s}=\phi_{c^{*}s}^{x_{ils}}(1-\phi_{c^{*}s})^{1-x_{ils}}$ for Bernoulli and Binomial cases respectively.

For the Bernoulli and Binomial cases, we have, respectively.:

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\phi_{c^{*}},\boldsymbol\theta_{l})&\propto&\theta_{lc^{*}}\phi_{c^{*}s}^{y_{ls}}(1-\phi_{c^{*}s})^{1-y_{ls}}
\end{array}
\label{ap:eq0003} 
\end{equation*}

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\phi_{c^{*}},\boldsymbol\theta_{l})&\propto&\theta_{lc^{*}}\phi_{c^{*}s}^{x_{ils}}(1-\phi_{c^{*}s})^{1-x_{ils}}
\end{array}
\label{ap:eq0004} 
\end{equation*}

\noindent where each cluster label can be draw from a categorical distribution with probabilities equal to (Multinomial, Bernoulli and Binomial):

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\phi_{c^{*}},\boldsymbol\theta_{l})&=&\frac{\theta_{lc^{*}}\phi_{c^{*}s}}{\displaystyle\sum_{c=1}^{C}\theta_{lc}\phi_{cs}}
\end{array}
\label{ap:eq0005} 
\end{equation*}

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\phi_{c^{*}},\boldsymbol\theta_{l})&=&\frac{\theta_{lc^{*}}\phi_{c^{*}s}^{y_{ls}}(1-\phi_{c^{*}s})^{1-y_{ls}}}{\displaystyle\sum_{c=1}^{C}\theta_{lc}\phi_{cs}^{y_{ls}}(1-\phi_{cs})^{1-y_{ls}}}
\end{array}
\label{ap:eq0006} 
\end{equation*}

\begin{equation*}
\begin{array}{lll}
p(Z_{lc}=c^{*}|Y_{ls}=s,\boldsymbol\phi_{c^{*}},\boldsymbol\theta_{l})&=&\frac{\theta_{lc^{*}}\phi_{c^{*}s}^{x_{ils}}(1-\phi_{c^{*}s})^{1-x_{ils}}}{\displaystyle\sum_{c=1}^{C}\theta_{lc}\phi_{cs}^{x_{ils}}(1-\phi_{cs})^{1-x_{ils}}}
\end{array}
\label{ap:eq0007} 
\end{equation*}

For the $\boldsymbol\phi_{c^{*}}$ its Full Conditional is given by:

\begin{equation*}
\begin{array}{lll}
p\left(\boldsymbol\phi_{c^{*}}|\mathbf{Z}_{\cdot c^{*}},\{\mathbf{Y}_{l\cdot}|Z_{lc}=c^{*}\},\boldsymbol\beta\right)&\propto& \displaystyle\prod_{l=1}^{L}\phi_{c^{*}1}^{\mathbbm{1}(Y_{l1}=1,Z_{lc}=c^{*})}\times\dots\times\phi_{c^{*}S}^{\mathbbm{1}(Y_{l1}=S,Z_{lc}=c^{*})}\\[.5em]
&\times&\phi_{c^{*}1}^{\beta_{1}-1}\times\dots\times\phi_{c^{*}S}^{\beta_{S}-1}\\[.5em]
&\propto&\displaystyle\prod_{s=1}^{S}\phi_{c^{*}s}^{n_{c^{*}s}+\beta_{s}-1}
\end{array}
\label{ap:eq0005} 
\end{equation*}

\noindent where $n_{c^{*}s}$ is the total number of elements classified in community $c^{*}$ and independent variable $s$. In the Binomial or Bernoulli case the Full Conditional is given by:

\begin{equation*}
\begin{array}{lll}
p\left(\phi_{c^{*}s}|\mathbf{Z}_{\cdot c^{*}},\{\mathbf{Y}_{l\cdot}|Z_{lc}=c^{*}\},\boldsymbol\beta\right)&\propto&\displaystyle\prod_{l=1}^{L}\phi_{c^{*}s}^{\mathbbm{1}(Y_{ls}=s,Z_{lc}=c^{*})}\phi_{c^{*}s}^{\alpha_{0}-1}(1-\phi_{c^{*}s})^{\alpha_{1}-1}\\[.5em]
&\propto&\phi_{c^{*}s}^{n_{c^{*}s}+\alpha_{0}-1}(1-\phi_{c^{*}s})^{n_{-c^{*}s}+\alpha_{1}-1}
\end{array}
\label{ap:eq0006} 
\end{equation*}

\noindent where $n_{-c^{*}s}$ is the total number of elements that **not** belong to either community $c^{*}$ and independent variable $s$.

The last Full Conditional described here is the Full Conditional for $p\left(V_{lc}|Z_{lc}\right)$, in this case we have:

\begin{equation*}
\begin{array}{lll}
p\left(V_{lc}|Z_{lc}\right)&\propto& Binom(n_{lc}|N=N_{lc}+N_{l(c>c^{*})},V_{lc})Beta(V_{lc}|1,\gamma)\\[.5em]
&\propto&Beta(1+N_{lc},\gamma+N_{l(c>c^{*})})\\[.5em]
&=&\frac{\Gamma(N_{lc}+1+N_{l(c>c^{*})}+\gamma)}{\Gamma(N_{lc}+1)\Gamma(N_{l(c>c^{*})}+\gamma)}V_{lc}^{N_{lc}}\left(1-V_{lc}^{N_{l(c>c^{*})}+\gamma}\right)
\end{array}
\label{ap:eq0007} 
\end{equation*}

\begin{sidewaysfigure}[ht]
    \includegraphics[width=\linewidth, height=0.85\textheight]{plot0021.pdf}
    \caption{Clusters 1 - Iquitos-Nauta road.}
    \label{figure:cluster1}
\end{sidewaysfigure}

\begin{sidewaysfigure}[ht]
    \includegraphics[width=\linewidth, height=0.85\textheight]{plot0022.pdf}
    \caption{Clusters 2 - Iquitos-Nauta road.}
    \label{figure:cluster2}
\end{sidewaysfigure}

\begin{sidewaysfigure}[ht]
    \includegraphics[width=\linewidth, height=0.85\textheight]{plot0023.pdf}
    \caption{Clusters 3 - Iquitos-Nauta road.}
    \label{figure:cluster3}
\end{sidewaysfigure}

\begin{sidewaysfigure}[ht]
    \includegraphics[width=\linewidth, height=0.85\textheight]{plot0024.pdf}
    \caption{Clusters 4 - Iquitos-Nauta road.}
    \label{figure:cluster4}
\end{sidewaysfigure}

\begin{sidewaysfigure}[ht]
    \includegraphics[width=\linewidth, height=0.85\textheight]{plot0025.pdf}
    \caption{Clusters 5 - Iquitos-Nauta road.}
    \label{figure:cluster5}
\end{sidewaysfigure}


#References.
